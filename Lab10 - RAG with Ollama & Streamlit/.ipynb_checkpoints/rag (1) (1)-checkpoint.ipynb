{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30718e76-0333-4714-a864-d50ed661e418",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation With Ollama and Langchain\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a powerful approach that combines retrieval-based methods with generative models to provide contextually relevant and informative answers. In this notebook, we use LangChain's ecosystem to set up a conversational RAG system that uses documents stored as embeddings for rapid retrieval and accurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2aa72d-84c8-4186-9d40-c815b0832976",
   "metadata": {},
   "source": [
    "## Lab Description\n",
    "\n",
    "In this lab, you will build a Conversational Retrieval-Augmented Generation (RAG) system using LangChain and Ollama. The system retrieves relevant context from PDF documents, processes it with Mistral-7B, and generates concise, context-aware answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9133db38-edae-4c5b-a293-f803a9bf6940",
   "metadata": {},
   "source": [
    "## Lab Objectives:\n",
    "\n",
    "After Completing the Lab, Participants will be able to:\n",
    "\n",
    "- Load and process PDF documents into embeddings using FAISS.\n",
    "- Implement a history-aware retriever to enhance conversational understanding.\n",
    "- Construct a RAG pipeline for efficient document-based Q&A.\n",
    "- Run an interactive chat loop that enables dynamic question-answering based on document content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e86c7-9741-4dea-9ce0-07ee4f6dcf1c",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2291cf43-28fe-480d-9aab-bfef9797cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db94eb6-8c31-439d-a7ef-57cb8ac3f7d5",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) Workflow\n",
    "\n",
    "### Document Processing Pipeline\n",
    "The following image represents the **document processing pipeline** in a **Retrieval-Augmented Generation (RAG) system**. The pipeline consists of four main stages:\n",
    "\n",
    "1. **Load**: Raw documents in various formats (JSON, PDFs, URLs, etc.) are ingested.\n",
    "2. **Split**: The documents are chunked into smaller pieces to facilitate efficient retrieval.\n",
    "3. **Embed**: Each chunk is converted into numerical representations (embeddings) using an embedding model.\n",
    "4. **Store**: The embeddings are stored in a vector database, such as FAISS, for fast retrieval.\n",
    "\n",
    "This process ensures that information is structured efficiently for retrieval-based generation.\n",
    "\n",
    "<img src=\"one.png\" alt=\"Document Processing Pipeline\" width=\"800\">\n",
    "\n",
    "### Retrieval & Response Generation\n",
    "This image illustrates how a **Retrieval-Augmented Generation (RAG) pipeline** answers user queries:\n",
    "\n",
    "1. **User Question**: A query is input by the user.\n",
    "2. **Retrieve**: The system fetches relevant document chunks from the vector store based on semantic similarity.\n",
    "3. **Prompt**: The retrieved context is combined into a structured prompt.\n",
    "4. **LLM (Large Language Model)**: The prompt is sent to an LLM (e.g., Mistral, GPT) to generate an informed response.\n",
    "5. **Answer**: The final answer is provided to the user.\n",
    "\n",
    "This process ensures that responses are grounded in factual and retrieved data rather than purely relying on the model's pre-trained knowledge.\n",
    "\n",
    "<img src=\"two.png\" alt=\"RAG Retrieval & Response Generation\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b35add-bf27-491d-af9f-85d9df9d1547",
   "metadata": {},
   "source": [
    "## Allow nested asynchronous loops\n",
    "\n",
    "Jupyter notebooks already have an event loop running in the background, making it challenging to run asynchronous code directly. `nest_asyncio.apply()` resolves this by allowing asynchronous code to run within a notebook cell, even if the loop is already active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56acc7e4-5efc-49d9-96a8-827600b6810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740fe2e-4943-42fe-8558-93f08df83803",
   "metadata": {},
   "source": [
    "## The `get_conversational_answer` Function\n",
    "\n",
    "### Contextualizing the Question\n",
    "- The function starts by setting up a `contextualize_q_system_prompt`, which is a system instruction that reformulates the user's question based on the chat history. This step ensures that questions referencing past conversation context are rewritten as standalone questions that can be understood without that context.\n",
    "- The prompt is then fed into a `ChatPromptTemplate`, which organizes the messages for the language model. It includes placeholders for the system message, chat history, and user input.\n",
    "\n",
    "### Creating a History-Aware Retriever\n",
    "- `mistral:7b` is initialized as the LLM.\n",
    "- Using this LLM, `create_history_aware_retriever` is called, which combines the LLM with a retriever (a tool that fetches relevant documents). This retriever will be context-aware, ensuring conversational flow.\n",
    "\n",
    "### Setting Up the Question-Answering (QA) System Prompt\n",
    "- The `qa_system_prompt` is another system message that directs the assistant to answer the question concisely and to only respond if it has enough information.\n",
    "- A second `ChatPromptTemplate` is created to format these QA instructions, integrating context, chat history, and user input.\n",
    "\n",
    "### Creating the RAG Chain\n",
    "- A `question_answer_chain` is created using `create_stuff_documents_chain`, which combines the LLM and the QA prompt. This chain processes the retrieved documents (context) and provides answers.\n",
    "- Next, `create_retrieval_chain` links the history-aware retriever and the question-answering chain to form a RAG pipeline. The pipeline retrieves relevant context from documents and uses it to generate concise and precise answers.\n",
    "\n",
    "### Generating the Answer\n",
    "- The `rag_chain.invoke` method is called with the user input and chat history, returning a response (`ai_msg`) from the RAG pipeline. This response is structured to provide clear, contextually accurate answers based on both the user’s question and the retrieved documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf64f494-507f-416e-b55d-bfdd8d5b454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_conversational_answer(retriever, input, chat_history):\n",
    "    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "    which might reference context in the chat history, formulate a standalone question \\\n",
    "    which can be understood without the chat history. Do NOT answer the question, \\\n",
    "    just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = ChatOllama(model=\"llama3.1:8b\", base_url=\"http://10.79.253.112:11434\")\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Use three sentences maximum and keep the answer concise.\\\n",
    "    Do not generate any additional text unless you are asked to.\\\n",
    "    Keep the answers really short and concise.\\\n",
    "\n",
    "    {context}\"\"\"\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    ai_msg = rag_chain.invoke({\"input\": input, \"chat_history\": chat_history})\n",
    "    return ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756bb32-3f1d-4d2e-91aa-66cd9ec6c25a",
   "metadata": {},
   "source": [
    "## The `main` Function \n",
    "\n",
    "The `main` function initiates the conversational question-answering chain based on PDF documents stored in the specified directory.\n",
    "\n",
    "### PDF Document Loading\n",
    "- The directory containing PDF files is specified (`./data`), and these documents are loaded using `PyPDFDirectoryLoader`.\n",
    "- `documents` stores the loaded PDF data, which will be converted into embeddings for retrieval.\n",
    "\n",
    "### Vector Store Initialization\n",
    "- `OllamaEmbeddings` (using the \"mistral:7b\" model) is used to generate embeddings for the documents, which allows for semantic similarity searching.\n",
    "- Facebook AI Similarity Search (FAISS) is an open-source library that helps developers search for similar multimedia documents in large datasets. It stores these document embeddings, enabling quick retrieval based on the user's questions.\n",
    "- The vector store’s `as_retriever()` method provides a retriever object for retrieving relevant document chunks.\n",
    "\n",
    "### Conversation State Initialization\n",
    "- `chat_history` is initialized as an empty list to store user inputs and assistant responses. This is later used for reformulating user questions to enable contextual question answering. \n",
    "\n",
    "### Interactive Question-Answer Loop\n",
    "- A loop takes user input (prompt) to ask questions based on the uploaded PDF documents.\n",
    "- The loop breaks if the user types \"exit\".\n",
    "\n",
    "### Getting the AI Response\n",
    "- The `get_conversational_answer` function is called using `asyncio.run()`, taking in the retriever, user prompt, and chat history to generate contextually relevant responses.\n",
    "- The AI’s answer (`ai_msg[\"answer\"]`) and the user’s question are added to `chat_history` for providing context in the future responses.\n",
    "\n",
    "### Displaying the Assistant’s Response\n",
    "- The assistant’s response is printed to the console.\n",
    "- This loop continues until the user types in 'exit'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b62c52b-aa2a-4b5a-b0c8-d3c188d2072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Specify the directory where the PDF is stored\n",
    "    pdf_directory = \"./data\"\n",
    "\n",
    "    # Load the PDF documents\n",
    "    loader = PyPDFDirectoryLoader(pdf_directory)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Initialize the vector store using the embeddings model\n",
    "    embed_model = OllamaEmbeddings(model='nomic-embed-text:latest', base_url=\"http://10.79.253.112:11434\")\n",
    "    vector_store = FAISS.from_documents(documents, embed_model)\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    # Initialize the conversation state\n",
    "    chat_history = []\n",
    "\n",
    "    while True:\n",
    "        # Take user input for a question\n",
    "        prompt = input(\"Ask your question based on the uploaded PDF (or type 'exit' to quit): \")\n",
    "\n",
    "        if prompt.lower() == 'exit':\n",
    "            print(\"Exiting the conversation.\")\n",
    "            break\n",
    "\n",
    "        # Get the AI response using the retriever and chain\n",
    "        ai_msg = asyncio.run(get_conversational_answer(retriever, prompt, chat_history))\n",
    "\n",
    "        # Store the user input and AI response in the chat history\n",
    "        chat_history.extend([HumanMessage(content=prompt), ai_msg[\"answer\"]])\n",
    "\n",
    "        # Display the assistant's response\n",
    "        print(\"Assistant: \", ai_msg[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba129623-b95a-4a1a-947d-5dabbb355b84",
   "metadata": {},
   "source": [
    "## Call the `main` function to initiate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa83809-81ca-439b-a1bd-046725b97dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question based on the uploaded PDF (or type 'exit' to quit):  what is the paper about ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zv/p5w561lj289dlhpwttgbh84c0000gn/T/ipykernel_15511/1120438468.py:14: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"gemma2:2b\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  The provided context discusses attention mechanisms in neural networks, particularly focusing on their applications in natural language processing (NLP). It specifically mentions:\n",
      "\n",
      "* **Attention mechanism:** A method to weigh the importance of different parts of the input sequence for generating an output. \n",
      "* **Contextual representation:**  Neural networks learn representations that take into account the relationship between words and phrases based on surrounding context. This is achieved through attention mechanisms.\n",
      "* **Applications in NLP:** The paper highlights how attention mechanisms are used to improve natural language understanding, translation, and summarization tasks.\n",
      "\n",
      "The context suggests a deep dive into the workings of these neural network techniques and their contributions to NLP research and applications. \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question based on the uploaded PDF (or type 'exit' to quit):  explain more about attention mechanism \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  Let's delve into the fascinating world of attention mechanisms!\n",
      "\n",
      "**The Essence of Attention: Focusing on Relevant Information**\n",
      "\n",
      "Imagine you're reading a book. You don't process every word equally—you focus on key phrases, words related to your current understanding, and important entities.  This is what attention does for AI models. It helps them determine which parts of the input data are most relevant for a specific task at hand.\n",
      "\n",
      "**How Attention Works: A Step-by-Step Guide**\n",
      "\n",
      "1. **Input Representation:** The model starts with a raw sequence (like words in a sentence) and encodes it into a representation called an embedding, essentially turning each word into a vector. \n",
      "2. **Query, Key, Value Matrices:** The encoding is transformed through multiple matrices: the query (Q), key (K), and value (V) matrices. Think of these as a set of spotlights that focus on specific parts of the input sequence.\n",
      "3. **Attention Score Calculation:**  The attention mechanism calculates an \"attention score\" for each word in the sequence based on its relationship to other words. The higher the score, the more relevant a word is. This happens using a dot product or scaled dot-product. \n",
      "4. **Weighted Sum:** Each word's relevance is weighted and summed up using the attention scores to produce a \"context vector,\" a single vector that represents the combined influence of all words in the sequence.  This vector contains information relevant to the current task.\n",
      "\n",
      "**Types of Attention Mechanisms: Choosing the Right Focus**\n",
      "\n",
      "1. **Self-Attention:** This is used for understanding relationships within a sentence or document, like figuring out how different words relate to each other. Think about summarizing sentences or translating paragraphs.\n",
      "   * The model attends to all words in a sequence, which helps it grasp dependencies and long-range connections between them. \n",
      "2. **Multi-Head Attention:**  This uses multiple self-attention mechanisms (each focusing on slightly different aspects of the input). This allows for more nuanced understanding by capturing diverse relationships and allowing the model to learn a richer representation.\n",
      "3. **Masked Self-Attention:** Used in tasks like language modeling, where attention should only look at previously encountered words. Prevents the model from \"looking ahead\" too far. \n",
      "\n",
      "\n",
      "**Applications of Attention:**\n",
      "\n",
      "* **Machine Translation:**  Translating between languages by paying attention to the most relevant parts of both source and target sentences (e.g., Google Translate).\n",
      "* **Text Summarization:**  Creating concise summaries by identifying key words and phrases in a long text (e.g., creating bullet points, summarizing articles).\n",
      "* **Natural Language Processing (NLP):** Understanding the context of words within a sentence for tasks like sentiment analysis, question answering, and intent recognition. \n",
      "* **Speech Recognition:** Focusing on important sounds during speech to better understand spoken language (e.g., voice assistants). \n",
      "\n",
      "\n",
      "**Benefits of Attention:**\n",
      "\n",
      "* **Improved Accuracy:**  Attention mechanisms enhance model performance by enabling them to focus on crucial information, leading to more accurate predictions.\n",
      "* **Long-Range Dependencies:** They are especially good at capturing long-range dependencies between words in a sequence (e.g., understanding how a word's meaning depends on its neighbors). \n",
      "\n",
      "**In Summary:** Attention is a powerful tool for AI models that allows them to focus on relevant information, leading to improved performance in various tasks.  \n",
      "\n",
      "\n",
      "Let me know if you have any specific questions or would like to explore different applications of attention mechanisms!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question based on the uploaded PDF (or type 'exit' to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the conversation.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a5fe4-5e73-4749-be9a-720986a4322b",
   "metadata": {},
   "source": [
    "## Integration with Streamlit UI \n",
    "\n",
    "Run this cell to copy the entire code to a `.py` named `app.py`. Launch a new terminal an type `streamlit run app.py` to see the entire rag system demonstarted above with an interactive UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c15a2a-53d1-4730-8804-fa9f3eb9d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./app.py\n",
    "\n",
    "import streamlit as st\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def get_conversational_answer(retriever,input,chat_history):\n",
    "    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "    which might reference context in the chat history, formulate a standalone question \\\n",
    "    which can be understood without the chat history. Do NOT answer the question, \\\n",
    "    just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Use three sentences maximum and keep the answer concise.\\\n",
    "    Donot generate any additional text unless you are asked to.\\\n",
    "    Keep the answers really short and concise.\\\n",
    "\n",
    "    {context}\"\"\"\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    ai_msg = rag_chain.invoke({\"input\": input, \"chat_history\": chat_history})\n",
    "    return  ai_msg\n",
    "\n",
    "\n",
    "def main():\n",
    "    st.header('Chat with your PDF')\n",
    "    \n",
    "    if \"conversation\" not in st.session_state:\n",
    "        st.session_state.conversation = None\n",
    "\n",
    "    if \"activate_chat\" not in st.session_state:\n",
    "        st.session_state.activate_chat = False\n",
    "\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "        st.session_state.chat_history=[]\n",
    "\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"], avatar = message['avatar']):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    embed_model = OllamaEmbeddings(model='mistral')\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.subheader('Upload Your PDF File')\n",
    "        docs = st.file_uploader('Upload your PDF & Click to process',accept_multiple_files = True, type=['pdf'])\n",
    "        if st.button('Process'):\n",
    "            if docs is not None:\n",
    "                os.makedirs('./data', exist_ok=True)\n",
    "                for doc in docs:\n",
    "                    save_path = os.path.join('./data', doc.name)\n",
    "                    with open(save_path, 'wb') as f:\n",
    "                        f.write(doc.getbuffer())\n",
    "                    st.write(f'Processed file: {save_path}')\n",
    "           \n",
    "            with st.spinner('Processing'):\n",
    "                loader = PyPDFDirectoryLoader(\"./data\")\n",
    "                documents = loader.load()\n",
    "                vector_store = FAISS.from_documents(documents, embed_model)\n",
    "                retriever=vector_store.as_retriever()\n",
    "                if \"retriever\" not in st.session_state:\n",
    "                    st.session_state.retriever = retriever\n",
    "                st.session_state.activate_chat = True\n",
    "\n",
    "            # Delete uploaded PDF files after loading\n",
    "            for doc in os.listdir('./data'):\n",
    "                os.remove(os.path.join('./data', doc))\n",
    "\n",
    "    if st.session_state.activate_chat == True:\n",
    "        if prompt := st.chat_input(\"Ask your question based on the uploaded PDF\"):\n",
    "            with st.chat_message(\"user\", avatar = '👨🏻'):\n",
    "                st.markdown(prompt)\n",
    "            st.session_state.messages.append({\"role\": \"user\",  \"avatar\" :'👨🏻', \"content\": prompt})\n",
    "            retriever = st.session_state.retriever\n",
    "\n",
    "            ai_msg = asyncio.run(get_conversational_answer(retriever,prompt,st.session_state.chat_history))\n",
    "            st.session_state.chat_history.extend([HumanMessage(content=prompt), ai_msg[\"answer\"]])\n",
    "            cleaned_response=ai_msg[\"answer\"]\n",
    "            with st.chat_message(\"assistant\", avatar='🤖'):\n",
    "                st.markdown(cleaned_response)\n",
    "            st.session_state.messages.append({\"role\": \"assistant\",  \"avatar\" :'🤖', \"content\": cleaned_response})\n",
    "        else:\n",
    "            st.markdown('Upload your PDFs to chat')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4383e2-4ad3-4e32-af0b-874b289d10a0",
   "metadata": {},
   "source": [
    "## Accessing the UI\n",
    "\n",
    "### **Step 1: Go to http://10.79.253.111:8501 on a new tab**\n",
    "The application starts with an interface where users can upload **PDF files**. The sidebar provides an option to **browse** and select files.\n",
    "\n",
    "<img src=\"1.png\" alt=\"Chat with Your PDF - Initial Interface\" width=\"800\">\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Selecting a PDF File**\n",
    "Users can browse their system and select a **PDF file** for processing.\n",
    "\n",
    "<img src=\"2.png\" alt=\"Selecting a PDF File\" width=\"800\">\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Uploading and Processing the PDF**\n",
    "After selecting the file, users click **\"Process\"** to upload and analyze the document.\n",
    "\n",
    "<img src=\"3.png\" alt=\"Uploading and Processing the PDF\" width=\"800\">\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Conversational Chat Based on PDF**\n",
    "Once processing is complete, users can chat with the PDF content by asking questions. The system retrieves relevant information and provides responses.\n",
    "\n",
    "<img src=\"4.png\" alt=\"Chatting with the PDF Document\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a8b47-84b1-4b43-b9ff-985245735b8c",
   "metadata": {},
   "source": [
    "## **Testing the RAG System with HPE ProLiant Compute DL380a Gen12 QuickSpecs**\n",
    "\n",
    "### **Instructions:**\n",
    "Use the following questions to test the **Retrieval-Augmented Generation (RAG) system**. These questions are designed to verify if the system can accurately retrieve and generate responses from the **HPE ProLiant Compute DL380a Gen12 QuickSpecs** document.\n",
    "\n",
    "### **Test Questions:**\n",
    "1. **What are the key features of the HPE ProLiant Compute DL380a Gen12 server?**\n",
    "2. **Which Intel® Xeon® 6 processors are supported by the HPE ProLiant Compute DL380a Gen12, and what are their specifications?**\n",
    "3. **What are the supported GPU configurations for the HPE ProLiant Compute DL380a Gen12, and what are the power requirements for different configurations?**\n",
    "\n",
    "### **How to Use:**\n",
    "- Upload the **QuickSpecs PDF** to the RAG system.\n",
    "- Ask each question one by one.\n",
    "- Verify if the response is **accurate and based on the document**.\n",
    "\n",
    " **If the system retrieves correct responses, the RAG pipeline is working effectively!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305bf94-38b1-426c-8cab-3d643e13c862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
