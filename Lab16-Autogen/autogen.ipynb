{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1122b6bf-d16f-4fd9-80e5-aadcfbf21c75",
   "metadata": {},
   "source": [
    "# Agentic AI with AutoGen\n",
    "\n",
    "Agentic AI refers to artificial intelligent systems that form an \"agency\". This enables them to perform specific tasks and make decisions to achieve specific goals. These AI systems can adapt to changes and execute decisions without continuos human interventions. \n",
    "\n",
    "AutoGen is an open-source framework that facilitates the creation and orchestration of Agentic AI systems. Autogen makes it possible to develop flexible multi agent systems that can interact with each other. These agents can operate both autonomousoly and with human oversight. \n",
    "\n",
    "AutoGen can: \n",
    "- Facilitate mathematical problem solving and code generation.\n",
    "- Enhance the quality and accuracy of information retrieval.\n",
    "- Optimize complex decision making processes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6e0d8f-3991-40db-9d69-b13382cc0326",
   "metadata": {},
   "source": [
    "## Lab Description:\n",
    "\n",
    "This lab explores agentic AI using AutoGen, showcasing how AI agents can autonomously interact, collaborate, and execute tasks. The lab begins by introducing basic conversable agents and making them communicate with each other. Next, we incorporate a human-in-the-loop to allow human participation in the conversation. We then demonstrate a Code Executor Agent, capable of running code autonomously. Finally, we set up a group chat involving multiple specialized agents, including a Coder Agent, Critique Agent, and User Proxy Agent, to showcase how agents can collaborate effectively on complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57023ff8-298b-4bb1-a0ba-54e29e5fc49e",
   "metadata": {},
   "source": [
    "## Lab Objectives\n",
    "\n",
    "- Understand the fundamentals of AutoGen and conversable AI agents.\n",
    "  \n",
    "- Enable AI agents to communicate and collaborate autonomously.\n",
    "  \n",
    "- Integrate a human-in-the-loop to participate in AI-driven conversations.\n",
    "  \n",
    "- Demonstrate a Code Executor Agent and a multi-agent group chat with specialized roles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8ad67-23ba-4828-8515-d7f4d571d740",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "In AutoGen, an agent is the entity that can send and recieve messages from other agents. An agent can be powered by humans, models or code executors. An example of a built in agent in AutoGen is the `ConversableAgent`.\n",
    "\n",
    "Let us build a simple 2 agent system that discusses about 'Large Language Models' with each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fb099-5efe-4b1f-9fa0-45be7b44460b",
   "metadata": {},
   "source": [
    "First step would be to configure the LLM. We use llama3.1:8b from ollama. We will have to edit the `config_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9d45ad-2033-49c2-bf7d-891534348dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        # Let's choose the Meta's Llama 3.1 model (model names must match Ollama exactly)\n",
    "        \"model\": \"llama3.1:8b\",\n",
    "        # We specify the API Type as 'ollama' so it uses the Ollama client class\n",
    "        \"api_type\": \"ollama\",\n",
    "        \"stream\": False,\n",
    "        #Specify the address where ollama is hosted\n",
    "        \"client_host\": \"http://10.79.253.114:11434\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18706548-7131-4890-b509-55f24720a049",
   "metadata": {},
   "source": [
    "Once we have our LLM configured, we are all set to initialize the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549c5eed-be91-4b96-b9c4-4a3c7b3fbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent\n",
    "\n",
    "luna = ConversableAgent(\n",
    "    \"luna\",\n",
    "    system_message=\"Your name is Luna and you are a part of a duo that is discussing about Large Language Models. Your conversation should be brief and concise.\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "zara = ConversableAgent(\n",
    "    \"zara\",\n",
    "    system_message=\"Your name is Zara and you are a part of a duo that is discussing about Large Language Models. Your conversation should be brief and concise.\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400e4e8-f1e6-4269-b583-d226381bc0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can initiate the conversation now that we have initialized the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9859e3-8fb9-4671-b853-dd4cc9c80e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "luna (to zara):\n",
      "\n",
      "Zara, tell me about the latest innovations in Large Language Models.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://10.79.253.114:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zara (to luna):\n",
      "\n",
      "We've been following the advancements closely. Our team has been working on integrating multimodal capabilities into our LLM architecture. We've seen significant improvements in understanding and generating text from images, videos, and other multimedia inputs.\n",
      "\n",
      "One notable breakthrough is the emergence of vision-and-language models that can not only comprehend text but also generate visual representations. This convergence of AI and computer vision will likely revolutionize various applications, from image description to content creation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://10.79.253.114:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "luna (to zara):\n",
      "\n",
      "That's exciting stuff! I've been digging into the implications for natural language processing. It seems like these multimodal models are pushing the boundaries of what we thought possible in terms of understanding context and nuances in human communication.\n",
      "\n",
      "I'm curious, have you explored any potential applications in the realm of accessibility? For example, how might these models be used to aid people with disabilities or those who struggle with language barriers?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://10.79.253.114:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zara (to luna):\n",
      "\n",
      "Our team has indeed been exploring the accessibility aspect. One area we're looking into is real-time translation and interpretation for individuals with hearing or speech impairments. We're also experimenting with models that can generate text-to-speech synthesis, enabling more effective communication between people with visual or cognitive disabilities.\n",
      "\n",
      "Moreover, these multimodal LLMs could potentially facilitate language learning by providing immersive, interactive experiences that simulate conversations in different languages and cultures. It's an area where we believe technology can greatly improve lives and bridge cultural divides.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = luna.initiate_chat(zara, message=\"Zara, tell me about the latest innovations in Large Language Models.\", max_turns=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47af9a8-22f8-40be-9902-6e91477646e2",
   "metadata": {},
   "source": [
    "We have provided system instructions to both the agents that they are each part of a duo conversing about Large Language Models. We have powered them with the `llm_config`. During initiation we set `max_turns` to 2, which terminates the chat after two turns.\n",
    "\n",
    "### The flow is summarized by the following diagram:\n",
    "<br><br>\n",
    "<img src=\"./dia#1.jpg\" alt=\"Alt Text\" width=\"650\" height=\"650\" style=\"display: block; margin: 0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b035b1-06f0-4fea-a558-9b220e1b7151",
   "metadata": {},
   "source": [
    "## Human in the loop \n",
    "\n",
    "So what if you want to converse with an agent ?, say you want to talk with Zara about LLMs. You can initialize an agent powered by human. You can do this by setting `human_input_mode=\"ALWAYS\"` in the agent's initialization. This will prompt you for an input at every turn of the conversation. Let's see this in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d2cc25-5d67-4fdf-a829-41aeb61ab1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "zara = ConversableAgent(\n",
    "    \"zara\",\n",
    "    system_message=\"Your name is Zara and you are a part of a duo that is discussing about Large Language Models. Your conversation should be brief and concise.\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "human = ConversableAgent(\n",
    "     \"human\",\n",
    "     llm_config=False,    #no need to use LLM as the user provides the input\n",
    "     human_input_mode=\"ALWAYS\",  #prompts you for an input at every turn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c185328d-402d-4f89-a0c0-00928b025b4d",
   "metadata": {},
   "source": [
    "We are all set to initiate the conversation !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc34e649-1b7c-4293-bb76-d7f348a53ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human (to zara):\n",
      "\n",
      "Hey, Zara, have you heard about the transformers model ? \n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://10.79.253.114:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zara (to human):\n",
      "\n",
      "I'm familiar with it! My partner and I were just discussing its architecture yesterday. It's a type of encoder-decoder model that uses self-attention mechanisms to process sequential data. What's your interest in it? Are we going to dive into its applications or limitations?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Replying as human. Provide feedback to zara. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  what is the attention mechanism \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human (to zara):\n",
      "\n",
      "what is the attention mechanism \n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://10.79.253.114:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zara (to human):\n",
      "\n",
      "The attention mechanism! It's a key component of transformer models. In simple terms, it allows the model to focus on specific parts of the input data that are relevant to the task at hand.\n",
      "\n",
      "Think of it like reading a book: you don't read every word equally; you focus on the important sentences and skip the less relevant ones. The attention mechanism helps the model do something similar, weighing the importance of different words or tokens in the input sequence.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Replying as human. Provide feedback to zara. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mhuman\u001b[0m (to zara):\n",
      "\n",
      "i am not aware of any limitations as of now, it is really great, the attention mechanism blew my mind\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mzara\u001b[0m (to human):\n",
      "\n",
      "I know what you mean! The self-attention mechanism in transformers is a game-changer. It allows the model to weigh the importance of different input elements relative to each other, which is super powerful for capturing contextual relationships. We're seeing some impressive results with it too! Have you explored its applications in dialogue systems or text summarization?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replying as human. Provide feedback to zara. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  exit\n"
     ]
    }
   ],
   "source": [
    "result = human.initiate_chat(\n",
    "    zara,  # specify that the chat is with Zara\n",
    "    message=\"Hey, Zara, have you heard about the transformers model ? \",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65becbd-7406-489c-9999-c2300c82d1d0",
   "metadata": {},
   "source": [
    "### The flow can be summarized by the following diagram: \n",
    "<br>\n",
    "<img src=\"./dia#2u.png\" alt=\"Alt Text\" width=\"650\" height=\"650\" style=\"display: block; margin: 0 auto;\"/>\n",
    "<br><br>\n",
    "\n",
    "\n",
    "Human can either decide to reply, skip or exit. If human chooses to reply, the agent will continue the conversation accordingly. If the human decide to skip (press \"Enter\"), the agent will initiate auto reply. If human decides to exit, the conversation is terminated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a045db-bf6b-4436-ba3b-df3e8ce80f06",
   "metadata": {},
   "source": [
    "## Code Executors\n",
    "\n",
    "In AutoGen, a code executor is a component that takes in an input message which contain a code block, perform the execution and ouputs the result. Let us try to initialize a built-in command line code executor which executes code in the command line environment. \n",
    "\n",
    "When the executor recieves a code block, it first writes the code into a code file. Then it starts a subprocess to execute that code file. When it gets the console output, it finally reads the console output to give the final output message. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4562cc2-a057-497e-82e3-13e63505010a",
   "metadata": {},
   "source": [
    "First, we would want a temporary directory to store the code file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "803ae50d-cbd6-4c22-855e-55188a555d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# Create temporary directory to store the code file\n",
    "temp_dir = tempfile.TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a206b86-11c7-4a6f-98f6-d018cc954e46",
   "metadata": {},
   "source": [
    "Now we will initialize both the code executor agent and the local command line code executor. We will also pass the local command line code executor to the agent initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e49784a-a9b0-4b43-87ac-67d205f36e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "executor = LocalCommandLineCodeExecutor(\n",
    "    timeout=10,  # Timeout for each code execution in seconds.\n",
    "    work_dir=temp_dir.name,  # Use the temporary directory to store the code files.\n",
    ")\n",
    "\n",
    "# Create an agent with code executor configuration.\n",
    "code_executor_agent = ConversableAgent(\n",
    "    \"code_executor_agent\",\n",
    "    llm_config=False,  # Turn off LLM for this agent.\n",
    "    code_execution_config={\"executor\": executor},  # Use the local command line code executor.\n",
    "    human_input_mode=\"ALWAYS\",  # Always take human input for this agent for safety.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32148660-89fb-495e-a110-e6b6f6c17a12",
   "metadata": {},
   "source": [
    "Now we can pass a message with codeblock to the code executor agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ef03323-14a0-4c3b-ab07-f5b90f3649bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"This is a message with a code block.\n",
    "Code block:\n",
    "```python\n",
    "def fibonacci(n):\n",
    "    fib_sequence = [0, 1]\n",
    "    for i in range(2, n):\n",
    "        next_number = fib_sequence[i-1] + fib_sequence[i-2]\n",
    "        fib_sequence.append(next_number)\n",
    "    return fib_sequence[:n]\n",
    "\n",
    "# Print the first 5 Fibonacci numbers\n",
    "first_5_fibonacci = fibonacci(5)\n",
    "print(\"First 5 Fibonacci numbers:\", first_5_fibonacci)\n",
    "```\n",
    "End of Message\"\"\"\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90051a38-6ce6-4c77-81e7-bdc9d1d4eb93",
   "metadata": {},
   "source": [
    "Now we will make the executor generate a response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e722d47-6842-4de7-86cf-d885f511f39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Replying as code_executor_agent. Provide feedback to the sender. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "reply = code_executor_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": message}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee576fb-a62d-4615-a5fc-a4fd771cc51f",
   "metadata": {},
   "source": [
    "The model generated the output after executing the code using `LocalCommandLineCodeExecutor` from AutoGen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cadaab-f1a0-44ce-a140-d29e9160ba15",
   "metadata": {},
   "source": [
    "## Group Chat\n",
    "\n",
    "Group chats in AutoGen are multi-agent conversations. The `user -> coder -> critic` Group chat would be an excellent example. What happens in this Group chat is really simple. There is a user proxy agent who initiate a group chat and later on execute the code. If there is any error in the code, the user proxy agent will not be able to provide an output. First, the user proxy initiates a chat by giving a task. Then the coder agent provides code for the particular task. The code is then passed to the critic, who evaluates the code by searching for any logical, syntactical or conceptual erros. Then the user executes the code, if any errors are present, the error is outputted as the user proxy messsage. The coder agent analyses the response further increasing the quality of the code. You can set `max_turns` to control the number of times you want the turns to repeat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3768fc92-b5b2-4b23-82bb-203a3d88245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen \n",
    "\n",
    "llm_config = {\"config_list\": config_list}\n",
    "\n",
    "user = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"groupchat\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",  # the default assistant agent is capable of solving problems with code\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "critic = autogen.AssistantAgent(\n",
    "    name=\"Critic\",\n",
    "    system_message=\"\"\"Critic. You are a helpful assistant highly skilled in evaluating the quality of a given visualization code by providing a score from 1 (bad) - 10 (good) while providing clear rationale. YOU MUST CONSIDER VISUALIZATION BEST PRACTICES for each evaluation. Specifically, you can carefully evaluate the code across the following dimensions\n",
    "- bugs (bugs):  are there bugs, logic errors, syntax error or typos? Are there any reasons why the code may fail to compile? How should it be fixed? If ANY bug exists, the bug score MUST be less than 5.\n",
    "- Data transformation (transformation): Is the data transformed appropriately for the visualization type? E.g., is the dataset appropriated filtered, aggregated, or grouped  if needed? If a date field is used, is the date field first converted to a date object etc?\n",
    "- Goal compliance (compliance): how well the code meets the specified visualization goals?\n",
    "- Visualization type (type): CONSIDERING BEST PRACTICES, is the visualization type appropriate for the data and intent? Is there a visualization type that would be more effective in conveying insights? If a different visualization type is more appropriate, the score MUST BE LESS THAN 5.\n",
    "- Data encoding (encoding): Is the data encoded appropriately for the visualization type?\n",
    "- aesthetics (aesthetics): Are the aesthetics of the visualization appropriate for the visualization type and the data?\n",
    "\n",
    "YOU MUST PROVIDE A SCORE for each of the above dimensions.\n",
    "{bugs: 0, transformation: 0, compliance: 0, type: 0, encoding: 0, aesthetics: 0}\n",
    "Do not suggest code.\n",
    "Finally, based on the critique above, suggest a concrete list of actions that the coder should take to improve the code.\n",
    "\"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user, coder, critic], messages=[], max_round=8)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7e9c1-d39c-4f19-80b2-9056a69bc886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_proxy (to chat_manager):\n",
      "\n",
      "Write code for printing first 5 fibonacci numbers.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://10.79.253.114:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next speaker: Critic\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://10.79.253.114:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic (to chat_manager):\n",
      "\n",
      "I'll review some sample Python code for this task.\n",
      "\n",
      "```python\n",
      "def print_fibonacci(n):\n",
      "    fib = [0, 1]\n",
      "    while len(fib) < n:\n",
      "        fib.append(fib[-1] + fib[-2])\n",
      "    return fib[:n]\n",
      "\n",
      "print(print_fibonacci(5))\n",
      "```\n",
      "\n",
      "Now, let's evaluate this code against the criteria.\n",
      "\n",
      "**Bugs:** 10\n",
      "The code appears to be free of syntax errors and logical bugs. It correctly calculates the first `n` Fibonacci numbers.\n",
      "\n",
      "**Data transformation (transformation):** 8\n",
      "The data is transformed appropriately for the problem at hand. The function takes an integer `n` as input, uses a list to store the Fibonacci sequence up to length `n`, and then returns the list of the first `n` Fibonacci numbers.\n",
      "\n",
      "**Goal compliance (compliance):** 10\n",
      "The code meets its specified goal, which is to print the first 5 Fibonacci numbers. \n",
      "\n",
      "**Visualization type (type):** N/A (this task doesn't involve visualization)\n",
      "N/A\n",
      "\n",
      "**Data encoding (encoding):** 9\n",
      "The data is encoded in a way that makes sense for this problem: as integers in a list.\n",
      "\n",
      "**Aesthetics:** 9\n",
      "The code is well-structured and easy to read, making it aesthetically pleasing from an organization perspective. It uses clear variable names and includes comments.\n",
      "\n",
      "\n",
      "Given the critique above, here are some concrete actions that the coder should take:\n",
      "\n",
      "1. No immediate changes needed for bugs.\n",
      "2. Consider using a more efficient method of generating Fibonacci numbers (e.g., matrix exponentiation) if `n` is large, but this isn't an issue in this case since we're only calculating up to 5 numbers.\n",
      "3. Add comments or docstrings to describe what the function does and how it works.\n",
      "4. Consider adding input validation to ensure that `n` is a non-negative integer.\n",
      "5. If you plan on using this code for larger values of `n`, consider optimizing it further by storing only the last two Fibonacci numbers (since each new number is just the sum of these two) instead of calculating every number up to `n`.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://10.79.253.114:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next speaker: Critic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user.initiate_chat(\n",
    "    manager, \n",
    "    message = \"Write code for printing first 5 fibonacci numbers.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728843d-71c1-4272-a5c1-ede18243472c",
   "metadata": {},
   "source": [
    "### The basic workflow is shown using the following diagram: \n",
    "<br><br>\n",
    "<img src=\"./group_chat.png\" alt=\"Alt Text\" width=\"650\" height=\"650\" style=\"display: block; margin: 0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32731262-f0ca-4160-981a-309c3c7a4943",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"logo.png\" alt=\"flow\" width=\"150\" height=\"100\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c697e4-2c2c-4bf9-b849-62ac25d0619a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
